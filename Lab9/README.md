Модели с большим количеством узлов имеют способность переобучаться. В машинном обучении регуляризация — это способ предотвратить переобучение, добавляя штраф к функции потерь.

Главная идея Dropout - вместо обучения одной DNN обучить ансамбль нескольких DNN, а затем усреднить полученные результаты. 

**Dropout**  (a) Standart Neural Net  (b) After applying dropout
![image](https://github.com/MariaShaiina/Marathon_DL_SSAU_2023/assets/113552167/3edaa857-9bf5-4697-8aca-61d9f8f5b2c2)

С технической точки зрения, на каждом этапе обучения отдельные узлы либо исключаются из сети с вероятностью 1-p, либо сохраняются с вероятностью p.

Фаза обучения (b):

Фаза обучения: для каждого скрытого слоя, для каждой обучающей выборки, для каждой итерации игнорируйте (обнуляйте) случайную долю p узлов (и соответствующих активаций).

Этап тестирования (a):

Используйте все активации, но уменьшите их на коэффициент "p" (чтобы учесть недостающие активации во время обучения).
