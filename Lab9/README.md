Модели с большим количеством узлов имеют способность переобучаться. В машинном обучении регуляризация — это способ предотвратить переобучение, добавляя штраф к функции потерь.

Главная идея Dropout - вместо обучения одной DNN обучить ансамбль нескольких DNN, а затем усреднить полученные результаты. 

**Dropout**  (a) Standart Neural Net  (b) After applying dropout
![image](https://github.com/MariaShaiina/Marathon_DL_SSAU_2023/assets/113552167/3edaa857-9bf5-4697-8aca-61d9f8f5b2c2)

С технической точки зрения, на каждом этапе обучения отдельные узлы либо исключаются из сети с вероятностью 1-p, либо сохраняются с вероятностью p.

Фаза обучения (b):

Фаза обучения: для каждого скрытого слоя, для каждой обучающей выборки, для каждой итерации обнуляйте (dropping out) случайную долю p узлов (и соответствующих активаций).

Этап тестирования (a):

Используйте все активации, но уменьшите их на коэффициент "p" (чтобы учесть недостающие активации во время обучения).

Ссылки:
1. [Для чего нужен dropout в машинном обучении?](https://yandex.ru/q/machine-learning/10885269761/#:~:text=Dropout%20—%20это%20подход%20к%20регуляризации,узлов%20(и%20соответствующих%20активаций))
2. [Dropout — метод решения проблемы переобучения в нейронных сетях](https://habr.com/ru/companies/wunderfund/articles/330814/#)
3. [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/pdf/1207.0580.pdf)
4. [Распределе́ние Берну́лли](https://ru.wikipedia.org/wiki/Распределение_Бернулли#:~:text=Распределе́ние%20Берну́лли%20в%20теории%20вероятностей,известной%20вероятности%20успеха%20или%20неудачи.)
